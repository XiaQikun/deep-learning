{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#MNIST数据集相关得常数\n",
    "INPUT_NODE = 784        # 输入层的节点数。对于数据集，就等于图片的像素\n",
    "OUTPUT_NODE = 10        # 输出层的节点数。这个等于类别的数目，因为需要区分0~9这10个数字，所有输出层节点数为10\n",
    "\n",
    "# 配置神经网络的参数\n",
    "LAYER1_NODE = 500       # 隐藏层节点数。这里使用只有一个隐藏层的网络结果，这个隐藏层有500个节点\n",
    "BATCH_SIZE = 100        # 一个训练batch中数据集的个数。\n",
    "\n",
    "LEARNING_RATE_BASE = 0.8  # 基础的学习率\n",
    "LEARNING_RATE_DECAY = 0.99  # 学习的衰减率\n",
    "\n",
    "REGYLARIZATION_RATE = 0.0001  #描述模型复杂度的正则化项在损失函数中的系数\n",
    "TRAINING_STEPS = 30000     # 训练轮数\n",
    "MOVING_AVERAGE_DECAY = 0.99    # 滑动平均衰减率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 一个辅助函数，给定神经网络的输入和所有参数，计算神经网络的前向传播结果。在这里定义一个RELU激活函数的三层全连接神经网络。\n",
    "# 这个函数也支持传入用于计算参数平均值的类，这样方便在测试时使用滑动平均模型\n",
    "def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):\n",
    "    # 当没有提供滑动平均类时，直接使用参数当前的取值\n",
    "    if avg_class == None:\n",
    "        # 计算隐藏层的前向传播结果\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n",
    "        \n",
    "        # 计算输出层的前向传播结果。因为在计算损失函数的时候会一并计算softmax函数，\n",
    "        # 所以这里不需要加入激活函数。而且不加入softmax不会影响预测结果。因为预测时，\n",
    "        # 使用的是不同类别对应输出值的相对大小，有没有softmax层对最后分类结果的计算没影响。\n",
    "        # 于是在计算整个神经网络的前向传播时可以不加入最后的softmax层\n",
    "        return tf.matmul(layer1, weights2) + biases2\n",
    "    else:\n",
    "        # 首先使用avg_class.average函数计算出变量的滑动平均值\n",
    "        # 然后再计算相应的神经网络前向传播结果\n",
    "        layer1 = tf.nn.relu(tf.matmul(input_tensor,avg_class.average(weights1)) + avg_class.average(biases1))\n",
    "        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练模型的过程\n",
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n",
    "    \n",
    "    # 生成隐藏层的参数\n",
    "    weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n",
    "    biases1 = tf.Variable(tf.constant(0.1, shape=[ LAYER1_NODE]))\n",
    "    # 输出层的参数\n",
    "    weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n",
    "    biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "    \n",
    "    # 计算在当前参数下神经网络前向传播的结果。这里给出的用于计算滑动平均的类为None，\n",
    "    # 所以参数不会使用参数的滑动平均\n",
    "    y = inference(x, None, weights1, biases1, weights2, biases2)\n",
    "    \n",
    "    # 定义存储训练轮数的变量。这个变量不需要计算滑动平均值，所以这里指定这个变量为\n",
    "    # 不可训练的变量(trainable=False)。在使用Tensorflow训练神经网络时，\n",
    "    # 一般会将代表训练轮数的变量指定为不可训练的参数\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # 给定滑动平均衰减率和训练轮数的变量，初始化滑动平均类。\n",
    "    # 给定训练轮数的变量能够加快训练早期变量的更新速度\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    \n",
    "    # 在所有神经网络参数的变量上使用滑动平均。其它辅助变量（比如global_step）就不需要了。\n",
    "    # tf.trainable_variables返回的就是图上集合GraphKeys.TRAINABLE——VARIABLES中的元素。\n",
    "    # 这个集合的元素就是没指定trainable=False的参数\n",
    "    variable_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    \n",
    "    # 计算使用了滑动平均之后的前向传播结果\n",
    "    average_y = inference(x, variable_averages, weights1, biases1, weights2, biases2)\n",
    "     \n",
    "    # 计算交叉熵作为刻画预测值和真实值之间差距的损失函数。使用sparse_softmax_cross_entropy_with_logits计算\n",
    "    # 函数中第一个参数是神经网络不包括softmax层的前向传播结果，第二个是训练数据的正确答案\n",
    "    # 使用tf.argmax函数得到正确答案对应的类别标号\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    # 计算在当前batch中所有样例的交叉熵平均值\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    # 计算L2正则化损失\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(REGYLARIZATION_RATE)\n",
    "    # 计算模型的正则化损失\n",
    "    regularization = regularizer(weights1) + regularizer(weights2)\n",
    "    # 总损失等于\n",
    "    loss = cross_entropy_mean + regularization\n",
    "    # 设置指数衰减的学习率\n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, mnist.train.num_examples/BATCH_SIZE, MOVING_AVERAGE_DECAY)\n",
    "    \n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # 在训练神经网络模型时，每过一边数据既需要通过反向传播来更新神经网络的参数，\n",
    "    # 又要更新每一个参数的滑动平均。为了一次完成多个操作，Tensorflow提供了\n",
    "    # tf.congtrol_dependencies和tf.group两种机制。下面两行程序和\n",
    "    # train_op = tf.control_dependencies(train_step, variable_averages_op)\n",
    "    with tf.control_dependencies([train_step, variable_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "    \n",
    "    # 检验使用了滑动平均模型的神经网络前向传播结果是否正确.  tf.argmax(average_y, 1)\n",
    "    # tf.equal判断两个张量的每一维是否相等，如果相等，则返回True\n",
    "    correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_, 1))\n",
    "    # 下面这个运算首先将一个布尔型的数值转换为实型，然后计算平均值。这个平均值就是模型在这一组数据上的正确率\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # 初始化会话 开始训练过程\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        # 准备验证数据集\n",
    "        validate_feed = {x:mnist.validation.images, y_:mnist.validation.labels}\n",
    "        # 准备测试数据\n",
    "        test_feed = {x:mnist.test.images, y_:mnist.test.labels}\n",
    "        # 迭代训练神经网络\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            # 每1000轮输出一次在验证数据集上的预测结果\n",
    "            if i%1000 == 0:\n",
    "                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n",
    "                print(\"After %d training steps, validation accuracy using average model is %g\" % (i, validate_acc))\n",
    "\n",
    "            # 产生这一轮使用的一个batch的训练数据，并运行训练过程\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            sess.run(train_op, feed_dict={x:xs, y_:ys})\n",
    "\n",
    "        # 在训练结束后，在测试数据集上检测神经网络模型的最终正确率\n",
    "        test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print(\"After %d training steps, test accuracy using average model is %g\" % (i, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "After 0 training steps, validation accuracy using average model is 0.0632\n",
      "After 1000 training steps, validation accuracy using average model is 0.976\n",
      "After 2000 training steps, validation accuracy using average model is 0.9816\n",
      "After 3000 training steps, validation accuracy using average model is 0.9842\n",
      "After 4000 training steps, validation accuracy using average model is 0.9842\n",
      "After 5000 training steps, validation accuracy using average model is 0.9844\n",
      "After 6000 training steps, validation accuracy using average model is 0.9852\n",
      "After 7000 training steps, validation accuracy using average model is 0.9848\n",
      "After 8000 training steps, validation accuracy using average model is 0.9848\n",
      "After 9000 training steps, validation accuracy using average model is 0.9848\n",
      "After 10000 training steps, validation accuracy using average model is 0.985\n",
      "After 11000 training steps, validation accuracy using average model is 0.9852\n",
      "After 12000 training steps, validation accuracy using average model is 0.9856\n",
      "After 13000 training steps, validation accuracy using average model is 0.9848\n",
      "After 14000 training steps, validation accuracy using average model is 0.986\n",
      "After 15000 training steps, validation accuracy using average model is 0.9856\n",
      "After 16000 training steps, validation accuracy using average model is 0.9858\n",
      "After 17000 training steps, validation accuracy using average model is 0.9854\n",
      "After 18000 training steps, validation accuracy using average model is 0.9848\n",
      "After 19000 training steps, validation accuracy using average model is 0.9848\n",
      "After 20000 training steps, validation accuracy using average model is 0.9858\n",
      "After 21000 training steps, validation accuracy using average model is 0.9856\n",
      "After 22000 training steps, validation accuracy using average model is 0.9854\n",
      "After 23000 training steps, validation accuracy using average model is 0.9858\n",
      "After 24000 training steps, validation accuracy using average model is 0.9856\n",
      "After 25000 training steps, validation accuracy using average model is 0.9854\n",
      "After 26000 training steps, validation accuracy using average model is 0.9852\n",
      "After 27000 training steps, validation accuracy using average model is 0.9854\n",
      "After 28000 training steps, validation accuracy using average model is 0.9856\n",
      "After 29000 training steps, validation accuracy using average model is 0.9854\n",
      "After 29999 training steps, test accuracy using average model is 0.9838\n"
     ]
    }
   ],
   "source": [
    "#载入MNIST数据集\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True)\n",
    "train(mnist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
